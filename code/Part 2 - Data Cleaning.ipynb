{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7139bff",
   "metadata": {},
   "source": [
    "**Previous:** [Part 1 - Web Scraping](Part%201%20-%20Web%20Scraping.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36232af",
   "metadata": {},
   "source": [
    "<img src='http://imgur.com/1ZcRyrc.png' style='float: left; margin: 20px; height: 55px'>\n",
    "\n",
    "# Project 3: Reddit Web Scraping\n",
    "\n",
    "## Part 2 - Data Cleaning\n",
    "\n",
    "---\n",
    "## Contents\n",
    "---\n",
    "\n",
    "### [Part 1 - Web Scraping](Part%201%20-%20Web%20Scraping.ipynb)\n",
    "1. Introduction\n",
    "2. Import - Web Scraping using PRAW\n",
    "\n",
    "### [Part 2 - Data Cleaning](Part%202%20-%20Data%20Cleaning.ipynb)\n",
    "1. [Import](#1.-Import)\n",
    "2. [Cleaning - Data Frame and Text](#2.-Cleaning---Data-Frame-and-Text)\n",
    "\n",
    "### [Part 3 - Exploratory Data Analysis (EDA)](Part%203%20-%20Exploratory%20Data%20Analysis%20(EDA).ipynb)\n",
    "1. Import\n",
    "2. Exploratory Data Analysis - Trends\n",
    "3. Exploratory Data Analysis - Unigrams \n",
    "4. Exploratory Data Analysis - Bigrams\n",
    "5. Exploratory Data Analysis - Trigrams \n",
    "\n",
    "### [Part 4 - Pre-processing & Modelling](Part%204%20-%20Pre-processing%20&%20Modelling.ipynb)\n",
    "1. Import\n",
    "2. Pre-processing - Binarizing The 2 Classes, Train-test Split\n",
    "3. Modelling - Feature Engineering, Comparing Against Other Models\n",
    "4. Conclusion - Summary, Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c27cc6",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4d023b",
   "metadata": {},
   "source": [
    "### 1.1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0aeaf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'praw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpraw\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'praw'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import praw\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.util import bigrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9991a",
   "metadata": {},
   "source": [
    "> <font size = 3 color = \"crimson\"> Some of these imports are not needed for this book. While it's not important in this case, importing unnecessary things can slow your notebook down. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed6df87",
   "metadata": {},
   "source": [
    "### 1.2 Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feb437e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reddit_raw.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read raw data saved in previous notebook\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m reddit \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreddit_raw.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m reddit\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reddit_raw.csv'"
     ]
    }
   ],
   "source": [
    "# Read raw data saved in previous notebook\n",
    "reddit = pd.read_csv('reddit_raw.csv')\n",
    "\n",
    "reddit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ef2a1",
   "metadata": {},
   "source": [
    "> <font size = 3 color = \"crimson\"> File pathing error. Do be careful that your final file organisation matches your code. Fixed below. I note this is the 'final' dataset and not the raw one. Would be good if you included the raw dataset in your uploads as well.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d0ede5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>post_text</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>total_comments</th>\n",
       "      <th>post_url</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_type</th>\n",
       "      <th>title_&amp;_text</th>\n",
       "      <th>stemmed_round_1</th>\n",
       "      <th>lemmatized_round_1</th>\n",
       "      <th>stemmed_round_2</th>\n",
       "      <th>lemmatized_round_2</th>\n",
       "      <th>stemmed_round_3</th>\n",
       "      <th>lemmatized_round_3</th>\n",
       "      <th>trending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Daily Fasting Check-in!</td>\n",
       "      <td>* **Type** of fast (water, juice, smoking, etc...</td>\n",
       "      <td>16o7z6r</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.reddit.com/r/intermittentfasting/c...</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>hot</td>\n",
       "      <td>Daily Fasting Check-in! * **Type** of fast (wa...</td>\n",
       "      <td>['daili', 'fast', 'checkin', 'type', 'fast', '...</td>\n",
       "      <td>['daily', 'fasting', 'checkin', 'type', 'fast'...</td>\n",
       "      <td>['daili', 'fast', 'checkin', 'type', 'fast', '...</td>\n",
       "      <td>['daily', 'fasting', 'checkin', 'type', 'fast'...</td>\n",
       "      <td>['daili', 'fast', 'checkin', 'type', 'fast', '...</td>\n",
       "      <td>['daily', 'fasting', 'checkin', 'type', 'fast'...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I decided who I wanted to be and I became her 💅🏽</td>\n",
       "      <td>So a little background: I’m 39, have birthed t...</td>\n",
       "      <td>16ntqoy</td>\n",
       "      <td>1176</td>\n",
       "      <td>36</td>\n",
       "      <td>https://i.redd.it/fclkjnwhmgpb1.jpg</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>hot</td>\n",
       "      <td>I decided who I wanted to be and I became her ...</td>\n",
       "      <td>['decid', 'want', 'becam', 'littl', 'backgroun...</td>\n",
       "      <td>['decided', 'wanted', 'became', 'little', 'bac...</td>\n",
       "      <td>['decid', 'want', 'becam', 'background', '39',...</td>\n",
       "      <td>['decided', 'wanted', 'became', 'background', ...</td>\n",
       "      <td>['decid', 'want', 'becam', 'littl', 'backgroun...</td>\n",
       "      <td>['decided', 'wanted', 'became', 'little', 'bac...</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Some photos from a past vacation came up as a ...</td>\n",
       "      <td>I remember being miserable and insecure the en...</td>\n",
       "      <td>16ni914</td>\n",
       "      <td>1505</td>\n",
       "      <td>77</td>\n",
       "      <td>https://www.reddit.com/gallery/16ni914</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>hot</td>\n",
       "      <td>Some photos from a past vacation came up as a ...</td>\n",
       "      <td>['photo', 'past', 'vacat', 'came', 'memori', '...</td>\n",
       "      <td>['photo', 'past', 'vacation', 'came', 'memory'...</td>\n",
       "      <td>['photo', 'vacat', 'came', 'memori', 'today', ...</td>\n",
       "      <td>['photo', 'vacation', 'came', 'memory', 'today...</td>\n",
       "      <td>['photo', 'past', 'vacat', 'came', 'memori', '...</td>\n",
       "      <td>['photo', 'past', 'vacation', 'came', 'memory'...</td>\n",
       "      <td>115885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anybody find IF, lose weight, and then lose mo...</td>\n",
       "      <td>I know I am an idiot.</td>\n",
       "      <td>16nuqx9</td>\n",
       "      <td>198</td>\n",
       "      <td>78</td>\n",
       "      <td>https://www.reddit.com/r/intermittentfasting/c...</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>hot</td>\n",
       "      <td>Anybody find IF, lose weight, and then lose mo...</td>\n",
       "      <td>['anybodi', 'find', 'lose', 'lose', 'motiv', '...</td>\n",
       "      <td>['anybody', 'find', 'lose', 'lose', 'motivatio...</td>\n",
       "      <td>['anybodi', 'find', 'weight', 'motiv', 'know',...</td>\n",
       "      <td>['anybody', 'find', 'weight', 'motivation', 'k...</td>\n",
       "      <td>['anybodi', 'find', 'lose', 'weight', 'lose', ...</td>\n",
       "      <td>['anybody', 'find', 'lose', 'weight', 'lose', ...</td>\n",
       "      <td>15444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 and a half months of IF</td>\n",
       "      <td>From 234 to 211 in 2.5 months. It works! Once ...</td>\n",
       "      <td>16nuxqs</td>\n",
       "      <td>180</td>\n",
       "      <td>12</td>\n",
       "      <td>https://i.redd.it/30yqmtsdvgpb1.jpg</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>hot</td>\n",
       "      <td>2 and a half months of IF From 234 to 211 in 2...</td>\n",
       "      <td>['2', 'half', '234', '211', '25', 'work', 'dis...</td>\n",
       "      <td>['2', 'half', '234', '211', '25', 'work', 'dis...</td>\n",
       "      <td>['2', 'half', 'month', '234', '211', '25', 'mo...</td>\n",
       "      <td>['2', 'half', 'month', '234', '211', '25', 'mo...</td>\n",
       "      <td>['2', 'half', 'month', '234', '211', '25', 'mo...</td>\n",
       "      <td>['2', 'half', 'month', '234', '211', '25', 'mo...</td>\n",
       "      <td>2160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                            Daily Fasting Check-in!   \n",
       "1   I decided who I wanted to be and I became her 💅🏽   \n",
       "2  Some photos from a past vacation came up as a ...   \n",
       "3  Anybody find IF, lose weight, and then lose mo...   \n",
       "4                          2 and a half months of IF   \n",
       "\n",
       "                                           post_text       id  score  \\\n",
       "0  * **Type** of fast (water, juice, smoking, etc...  16o7z6r      1   \n",
       "1  So a little background: I’m 39, have birthed t...  16ntqoy   1176   \n",
       "2  I remember being miserable and insecure the en...  16ni914   1505   \n",
       "3                              I know I am an idiot.  16nuqx9    198   \n",
       "4  From 234 to 211 in 2.5 months. It works! Once ...  16nuxqs    180   \n",
       "\n",
       "   total_comments                                           post_url  \\\n",
       "0               2  https://www.reddit.com/r/intermittentfasting/c...   \n",
       "1              36                https://i.redd.it/fclkjnwhmgpb1.jpg   \n",
       "2              77             https://www.reddit.com/gallery/16ni914   \n",
       "3              78  https://www.reddit.com/r/intermittentfasting/c...   \n",
       "4              12                https://i.redd.it/30yqmtsdvgpb1.jpg   \n",
       "\n",
       "             subreddit post_type  \\\n",
       "0  intermittentfasting       hot   \n",
       "1  intermittentfasting       hot   \n",
       "2  intermittentfasting       hot   \n",
       "3  intermittentfasting       hot   \n",
       "4  intermittentfasting       hot   \n",
       "\n",
       "                                        title_&_text  \\\n",
       "0  Daily Fasting Check-in! * **Type** of fast (wa...   \n",
       "1  I decided who I wanted to be and I became her ...   \n",
       "2  Some photos from a past vacation came up as a ...   \n",
       "3  Anybody find IF, lose weight, and then lose mo...   \n",
       "4  2 and a half months of IF From 234 to 211 in 2...   \n",
       "\n",
       "                                     stemmed_round_1  \\\n",
       "0  ['daili', 'fast', 'checkin', 'type', 'fast', '...   \n",
       "1  ['decid', 'want', 'becam', 'littl', 'backgroun...   \n",
       "2  ['photo', 'past', 'vacat', 'came', 'memori', '...   \n",
       "3  ['anybodi', 'find', 'lose', 'lose', 'motiv', '...   \n",
       "4  ['2', 'half', '234', '211', '25', 'work', 'dis...   \n",
       "\n",
       "                                  lemmatized_round_1  \\\n",
       "0  ['daily', 'fasting', 'checkin', 'type', 'fast'...   \n",
       "1  ['decided', 'wanted', 'became', 'little', 'bac...   \n",
       "2  ['photo', 'past', 'vacation', 'came', 'memory'...   \n",
       "3  ['anybody', 'find', 'lose', 'lose', 'motivatio...   \n",
       "4  ['2', 'half', '234', '211', '25', 'work', 'dis...   \n",
       "\n",
       "                                     stemmed_round_2  \\\n",
       "0  ['daili', 'fast', 'checkin', 'type', 'fast', '...   \n",
       "1  ['decid', 'want', 'becam', 'background', '39',...   \n",
       "2  ['photo', 'vacat', 'came', 'memori', 'today', ...   \n",
       "3  ['anybodi', 'find', 'weight', 'motiv', 'know',...   \n",
       "4  ['2', 'half', 'month', '234', '211', '25', 'mo...   \n",
       "\n",
       "                                  lemmatized_round_2  \\\n",
       "0  ['daily', 'fasting', 'checkin', 'type', 'fast'...   \n",
       "1  ['decided', 'wanted', 'became', 'background', ...   \n",
       "2  ['photo', 'vacation', 'came', 'memory', 'today...   \n",
       "3  ['anybody', 'find', 'weight', 'motivation', 'k...   \n",
       "4  ['2', 'half', 'month', '234', '211', '25', 'mo...   \n",
       "\n",
       "                                     stemmed_round_3  \\\n",
       "0  ['daili', 'fast', 'checkin', 'type', 'fast', '...   \n",
       "1  ['decid', 'want', 'becam', 'littl', 'backgroun...   \n",
       "2  ['photo', 'past', 'vacat', 'came', 'memori', '...   \n",
       "3  ['anybodi', 'find', 'lose', 'weight', 'lose', ...   \n",
       "4  ['2', 'half', 'month', '234', '211', '25', 'mo...   \n",
       "\n",
       "                                  lemmatized_round_3  trending  \n",
       "0  ['daily', 'fasting', 'checkin', 'type', 'fast'...         2  \n",
       "1  ['decided', 'wanted', 'became', 'little', 'bac...     42336  \n",
       "2  ['photo', 'past', 'vacation', 'came', 'memory'...    115885  \n",
       "3  ['anybody', 'find', 'lose', 'weight', 'lose', ...     15444  \n",
       "4  ['2', 'half', 'month', '234', '211', '25', 'mo...      2160  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read raw data saved in previous notebook\n",
    "reddit = pd.read_csv('../data/reddits.csv')\n",
    "\n",
    "reddit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91608c12",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "\n",
    "| Column                             | Datatype  | Explanation                                           |\n",
    "| ---------------------------------- | --------- | ----------------------------------------------------- |\n",
    "| **title**                          | object    | Title of the Reddit post                              |\n",
    "| **post_text**                      | object    | Text content of the Reddit post                       |\n",
    "| **id**                             | object    | Unique identifier for the post                        |\n",
    "| **score**                          | int64     | Score or upvotes of the post                          |\n",
    "| **total_comments**                 | int64     | Total number of comments on the post                  |\n",
    "| **post_url**                       | object    | URL of the post                                       |\n",
    "| **subreddit**                      | object    | Subreddit where the post was made                     |\n",
    "| **post_type**                      | object    | Type or format of the post                            |\n",
    "| **time_uploaded**                  | object    | Timestamp when the post was uploaded                  |\n",
    "| **title_&_text**                   | object    | Title and text content with punctuations removed      |\n",
    "| **title_text_stemmed**             | object    | Title and text content after stemming                 |\n",
    "| **title_text_lemmatized**          | object    | Title and text content after lemmatization            |\n",
    "| **stemmed_round_1**                | object    | Stemmed title and text after 1st round of feature engineering|\n",
    "| **lemmatized_round_1**             | object    | Lemmatized title and text after 1st round of feature engineering|\n",
    "| **stemmed_round_2**                | object    | Stemmed title and text after 2nd round of feature engineering|\n",
    "| **lemmatized_round_2**             | object    | Lemmatized title and text after 2nd round of feature engineering|\n",
    "| **stemmed_round_3**                | object    | Stemmed title and text after 3rd round of feature engineering|\n",
    "| **lemmatized_round_3**             | object    | Lemmatized title and text after 3rd round of feature engineering|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ffeadf",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Cleaning - Data Frame and Text\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0083f5e0",
   "metadata": {},
   "source": [
    "Cleaning was done in 2 sections:\n",
    "1. Cleaning the dataframe\n",
    "2. Cleaning the texts\n",
    "\n",
    "### 2.1 Cleaning The Dataframe\n",
    "The following steps were undertaken to clean the dataframe containing the raw data:\n",
    "1. Column Headers - Check for trailing space, lowercase and snakecase if needed\n",
    "2. Data Info (data type and null values) - Check data type (change data type if necessary) and null values (fill or drop null values where necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df91715d",
   "metadata": {},
   "source": [
    "#### 2.1.1 Column Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d9c524e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'post_text', 'id', 'score', 'total_comments', 'post_url',\n",
       "       'subreddit', 'post_type', 'title_&_text', 'stemmed_round_1',\n",
       "       'lemmatized_round_1', 'stemmed_round_2', 'lemmatized_round_2',\n",
       "       'stemmed_round_3', 'lemmatized_round_3', 'trending'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check column headers\n",
    "reddit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b114ab4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'post_text', 'id', 'score', 'total_comments', 'post_url',\n",
       "       'subreddit', 'post_type', 'time_uploaded'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No trailing space found, so just lowercase and snake-case column headers\n",
    "reddit.columns = [col.lower().replace(' ', '_') for col in reddit.columns]\n",
    "\n",
    "reddit.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dab359",
   "metadata": {},
   "source": [
    "#### 2.1.2 Data Info (data type and null values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79641364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3960 entries, 0 to 3959\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   title           3960 non-null   object\n",
      " 1   post_text       2496 non-null   object\n",
      " 2   id              3960 non-null   object\n",
      " 3   score           3960 non-null   int64 \n",
      " 4   total_comments  3960 non-null   int64 \n",
      " 5   post_url        3960 non-null   object\n",
      " 6   subreddit       3960 non-null   object\n",
      " 7   post_type       3960 non-null   object\n",
      " 8   time_uploaded   3960 non-null   object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 278.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check info\n",
    "reddit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "182002f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                0\n",
       "post_text         1464\n",
       "id                   0\n",
       "score                0\n",
       "total_comments       0\n",
       "post_url             0\n",
       "subreddit            0\n",
       "post_type            0\n",
       "time_uploaded        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check null values\n",
    "reddit.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ff775b",
   "metadata": {},
   "source": [
    "* 3960 posts scraped in total\n",
    "* All data are in the correct type, no need for any change of data type\n",
    "* 1464 null values found in the 'Post Text' column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ffc235",
   "metadata": {},
   "source": [
    "#### 2.1.3 Create a new column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d927fc2",
   "metadata": {},
   "source": [
    "Before we remove null values, we want to first add a new column 'title_&_text'.\n",
    "\n",
    "As there are people who post in subreddit titles rather than in post text to get more traction, posts with just text in the title and image only in the text are marked as null values. For instance: <br><br>\n",
    "\n",
    "<a href='https://www.reddit.com/r/intermittentfasting/comments/16shbmz/i_lost_120_lbsshe_lost_80_one_meal_a_day_from/'>\n",
    "    <figure>\n",
    "        <img src='https://preview.redd.it/cft42u8lso151.jpg?width=960&crop=smart&auto=webp&s=7dffccf1f5f70685154e59bddfc63ad84beaaf7b'/>\n",
    "        <center><figcaption>A typical image-only reddit post</figcaption></center>\n",
    "    </figure>\n",
    "</a><br>\n",
    "To get as many words as possible, we will be adding words in the 'title' and 'post_text' columns to form a 'title_&_text' column. But prior to that, we will first .fillna( ) the 'post_text' column so the resultant 'title_&_text' column do not contain any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eccd6e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>post_text</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>total_comments</th>\n",
       "      <th>post_url</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_type</th>\n",
       "      <th>time_uploaded</th>\n",
       "      <th>title_&amp;_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Does taking flavoured creatine break a fast?</td>\n",
       "      <td>Taking one scoop, roughly 3g. It has sucralose...</td>\n",
       "      <td>16shh83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/intermittentfasting/c...</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>new</td>\n",
       "      <td>2023-09-26 07:57:13</td>\n",
       "      <td>Does taking flavoured creatine break a fast? T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I lost 120 lbs.......she lost 80. One meal a d...</td>\n",
       "      <td></td>\n",
       "      <td>16shbmz</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>https://i.redd.it/cft42u8lso151.jpg</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>new</td>\n",
       "      <td>2023-09-26 07:46:54</td>\n",
       "      <td>I lost 120 lbs.......she lost 80. One meal a d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does fasting out of spite work?</td>\n",
       "      <td>We’ll see in 4 weeks when I go to a wedding wh...</td>\n",
       "      <td>16sfrlc</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.reddit.com/r/intermittentfasting/c...</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>new</td>\n",
       "      <td>2023-09-26 06:10:27</td>\n",
       "      <td>Does fasting out of spite work? We’ll see in 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Daily Fasting Check-in!</td>\n",
       "      <td>* **Type** of fast (water, juice, smoking, etc...</td>\n",
       "      <td>16sfl07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/intermittentfasting/c...</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>new</td>\n",
       "      <td>2023-09-26 06:00:31</td>\n",
       "      <td>Daily Fasting Check-in! * **Type** of fast (wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90 Days of Intermittent Fasting - IT WORKS!</td>\n",
       "      <td>Hi Everyone, \\n\\nToday was the 90th day of my ...</td>\n",
       "      <td>16sdl2e</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.reddit.com/r/intermittentfasting/c...</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>new</td>\n",
       "      <td>2023-09-26 04:10:24</td>\n",
       "      <td>90 Days of Intermittent Fasting - IT WORKS! Hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0       Does taking flavoured creatine break a fast?   \n",
       "1  I lost 120 lbs.......she lost 80. One meal a d...   \n",
       "2                    Does fasting out of spite work?   \n",
       "3                            Daily Fasting Check-in!   \n",
       "4        90 Days of Intermittent Fasting - IT WORKS!   \n",
       "\n",
       "                                           post_text       id  score  \\\n",
       "0  Taking one scoop, roughly 3g. It has sucralose...  16shh83      1   \n",
       "1                                                     16shbmz      6   \n",
       "2  We’ll see in 4 weeks when I go to a wedding wh...  16sfrlc      0   \n",
       "3  * **Type** of fast (water, juice, smoking, etc...  16sfl07      1   \n",
       "4  Hi Everyone, \\n\\nToday was the 90th day of my ...  16sdl2e     17   \n",
       "\n",
       "   total_comments                                           post_url  \\\n",
       "0               0  https://www.reddit.com/r/intermittentfasting/c...   \n",
       "1               1                https://i.redd.it/cft42u8lso151.jpg   \n",
       "2               2  https://www.reddit.com/r/intermittentfasting/c...   \n",
       "3               0  https://www.reddit.com/r/intermittentfasting/c...   \n",
       "4               8  https://www.reddit.com/r/intermittentfasting/c...   \n",
       "\n",
       "             subreddit post_type        time_uploaded  \\\n",
       "0  intermittentfasting       new  2023-09-26 07:57:13   \n",
       "1  intermittentfasting       new  2023-09-26 07:46:54   \n",
       "2  intermittentfasting       new  2023-09-26 06:10:27   \n",
       "3  intermittentfasting       new  2023-09-26 06:00:31   \n",
       "4  intermittentfasting       new  2023-09-26 04:10:24   \n",
       "\n",
       "                                        title_&_text  \n",
       "0  Does taking flavoured creatine break a fast? T...  \n",
       "1  I lost 120 lbs.......she lost 80. One meal a d...  \n",
       "2  Does fasting out of spite work? We’ll see in 4...  \n",
       "3  Daily Fasting Check-in! * **Type** of fast (wa...  \n",
       "4  90 Days of Intermittent Fasting - IT WORKS! Hi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill null values with empty strings in the 'title' and 'post_text' columns\n",
    "reddit['title'].fillna('', inplace=True)\n",
    "reddit['post_text'].fillna('', inplace=True)\n",
    "\n",
    "# Create new column 'title_&_text', an addition of words from the 'title' and 'post_text' columns\n",
    "reddit['title_&_text'] = reddit['title'] + ' ' + reddit['post_text']\n",
    "\n",
    "reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af3bed4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title             0\n",
       "post_text         0\n",
       "id                0\n",
       "score             0\n",
       "total_comments    0\n",
       "post_url          0\n",
       "subreddit         0\n",
       "post_type         0\n",
       "time_uploaded     0\n",
       "title_&_text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "reddit.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66bf3e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download('punkt')\n",
    "#Please uncomment the above if you haven't downloaded these libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae271d7",
   "metadata": {},
   "source": [
    "### 2.2 Cleaning The Texts\n",
    "A function is defined to run the following cleaning steps:\n",
    "* removes punctuation\n",
    "* tokenize\n",
    "* lowercase\n",
    "* removes duplicate content\n",
    "* removes stopwords\n",
    "* stemming\n",
    "* lemmatizing\n",
    "\n",
    "As part of feature engineering, we have added stopwords in three different rounds (one batch over another). The stopwords were determined based on the most common words used. They were removed to prevent our model from training words that are uesd excessively and become unmeaningful.\n",
    "\n",
    "The model scores for each round are presented in the next notebook (Part 3 - Pre-processing & Modelling) to determine which set of features to use eventually. The set of features with the highest score was chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c795095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for cleaning\n",
    "\n",
    "# And returns stemmed text in a column and lemmatized text in the next column\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def clean(text, custom_stopwords):\n",
    "\n",
    "    remove_punct = ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    tokenize = re.split('\\W+', remove_punct)\n",
    "    \n",
    "    lowercase = [word.lower() for word in tokenize]\n",
    "        \n",
    "    all_stopwords = stopwords.words('english') + custom_stopwords\n",
    "    no_stopwords = [word for word in lowercase if word not in all_stopwords]\n",
    "    \n",
    "    remove_dupli = list(set(no_stopwords))\n",
    "    \n",
    "    stemmed = [ps.stem(word) for word in no_stopwords]\n",
    "    \n",
    "    lemmatized = [wn.lemmatize(word) for word in no_stopwords]\n",
    "    \n",
    "    return pd.Series({'stemmed': stemmed, 'lemmatized': lemmatized})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ddbe17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>total_comments</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_type</th>\n",
       "      <th>title_&amp;_text</th>\n",
       "      <th>stemmed_round_1</th>\n",
       "      <th>lemmatized_round_1</th>\n",
       "      <th>stemmed_round_2</th>\n",
       "      <th>lemmatized_round_2</th>\n",
       "      <th>stemmed_round_3</th>\n",
       "      <th>lemmatized_round_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>new</td>\n",
       "      <td>Does taking flavoured creatine break a fast? T...</td>\n",
       "      <td>[take, flavour, creatin, break, fast, take, on...</td>\n",
       "      <td>[taking, flavoured, creatine, break, fast, tak...</td>\n",
       "      <td>[take, flavour, creatin, break, fast, take, on...</td>\n",
       "      <td>[taking, flavoured, creatine, break, fast, tak...</td>\n",
       "      <td>[take, flavour, creatin, break, fast, take, on...</td>\n",
       "      <td>[taking, flavoured, creatine, break, fast, tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>new</td>\n",
       "      <td>I lost 120 lbs.......she lost 80. One meal a d...</td>\n",
       "      <td>[lost, 120, lbsshe, lost, 80, one, day, ]</td>\n",
       "      <td>[lost, 120, lbsshe, lost, 80, one, day, ]</td>\n",
       "      <td>[lost, 120, lbsshe, lost, 80, one, meal, day, ]</td>\n",
       "      <td>[lost, 120, lbsshe, lost, 80, one, meal, day, ]</td>\n",
       "      <td>[lost, 120, lbsshe, lost, 80, one, meal, day, ]</td>\n",
       "      <td>[lost, 120, lbsshe, lost, 80, one, meal, day, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>new</td>\n",
       "      <td>Does fasting out of spite work? We’ll see in 4...</td>\n",
       "      <td>[fast, spite, see, 4, week, go, wed, bh, siste...</td>\n",
       "      <td>[fasting, spite, see, 4, week, go, wedding, bh...</td>\n",
       "      <td>[fast, spite, work, see, 4, week, go, wed, bh,...</td>\n",
       "      <td>[fasting, spite, work, see, 4, week, go, weddi...</td>\n",
       "      <td>[fast, spite, work, see, 4, week, go, wed, bh,...</td>\n",
       "      <td>[fasting, spite, work, see, 4, week, go, weddi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>new</td>\n",
       "      <td>Daily Fasting Check-in! * **Type** of fast (wa...</td>\n",
       "      <td>[daili, fast, checkin, type, fast, water, juic...</td>\n",
       "      <td>[daily, fasting, checkin, type, fast, water, j...</td>\n",
       "      <td>[daili, fast, checkin, type, fast, water, juic...</td>\n",
       "      <td>[daily, fasting, checkin, type, fast, water, j...</td>\n",
       "      <td>[daili, fast, checkin, type, fast, water, juic...</td>\n",
       "      <td>[daily, fasting, checkin, type, fast, water, j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>new</td>\n",
       "      <td>90 Days of Intermittent Fasting - IT WORKS! Hi...</td>\n",
       "      <td>[90, day, intermitt, fast, work, hi, everyon, ...</td>\n",
       "      <td>[90, day, intermittent, fasting, work, hi, eve...</td>\n",
       "      <td>[90, day, intermitt, fast, work, hi, everyon, ...</td>\n",
       "      <td>[90, day, intermittent, fasting, work, hi, eve...</td>\n",
       "      <td>[90, day, intermitt, fast, work, hi, everyon, ...</td>\n",
       "      <td>[90, day, intermittent, fasting, work, hi, eve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score  total_comments            subreddit post_type  \\\n",
       "0      1               0  intermittentfasting       new   \n",
       "1      6               1  intermittentfasting       new   \n",
       "2      0               2  intermittentfasting       new   \n",
       "3      1               0  intermittentfasting       new   \n",
       "4     17               8  intermittentfasting       new   \n",
       "\n",
       "                                        title_&_text  \\\n",
       "0  Does taking flavoured creatine break a fast? T...   \n",
       "1  I lost 120 lbs.......she lost 80. One meal a d...   \n",
       "2  Does fasting out of spite work? We’ll see in 4...   \n",
       "3  Daily Fasting Check-in! * **Type** of fast (wa...   \n",
       "4  90 Days of Intermittent Fasting - IT WORKS! Hi...   \n",
       "\n",
       "                                     stemmed_round_1  \\\n",
       "0  [take, flavour, creatin, break, fast, take, on...   \n",
       "1          [lost, 120, lbsshe, lost, 80, one, day, ]   \n",
       "2  [fast, spite, see, 4, week, go, wed, bh, siste...   \n",
       "3  [daili, fast, checkin, type, fast, water, juic...   \n",
       "4  [90, day, intermitt, fast, work, hi, everyon, ...   \n",
       "\n",
       "                                  lemmatized_round_1  \\\n",
       "0  [taking, flavoured, creatine, break, fast, tak...   \n",
       "1          [lost, 120, lbsshe, lost, 80, one, day, ]   \n",
       "2  [fasting, spite, see, 4, week, go, wedding, bh...   \n",
       "3  [daily, fasting, checkin, type, fast, water, j...   \n",
       "4  [90, day, intermittent, fasting, work, hi, eve...   \n",
       "\n",
       "                                     stemmed_round_2  \\\n",
       "0  [take, flavour, creatin, break, fast, take, on...   \n",
       "1    [lost, 120, lbsshe, lost, 80, one, meal, day, ]   \n",
       "2  [fast, spite, work, see, 4, week, go, wed, bh,...   \n",
       "3  [daili, fast, checkin, type, fast, water, juic...   \n",
       "4  [90, day, intermitt, fast, work, hi, everyon, ...   \n",
       "\n",
       "                                  lemmatized_round_2  \\\n",
       "0  [taking, flavoured, creatine, break, fast, tak...   \n",
       "1    [lost, 120, lbsshe, lost, 80, one, meal, day, ]   \n",
       "2  [fasting, spite, work, see, 4, week, go, weddi...   \n",
       "3  [daily, fasting, checkin, type, fast, water, j...   \n",
       "4  [90, day, intermittent, fasting, work, hi, eve...   \n",
       "\n",
       "                                     stemmed_round_3  \\\n",
       "0  [take, flavour, creatin, break, fast, take, on...   \n",
       "1    [lost, 120, lbsshe, lost, 80, one, meal, day, ]   \n",
       "2  [fast, spite, work, see, 4, week, go, wed, bh,...   \n",
       "3  [daili, fast, checkin, type, fast, water, juic...   \n",
       "4  [90, day, intermitt, fast, work, hi, everyon, ...   \n",
       "\n",
       "                                  lemmatized_round_3  \n",
       "0  [taking, flavoured, creatine, break, fast, tak...  \n",
       "1    [lost, 120, lbsshe, lost, 80, one, meal, day, ]  \n",
       "2  [fasting, spite, work, see, 4, week, go, weddi...  \n",
       "3  [daily, fasting, checkin, type, fast, water, j...  \n",
       "4  [90, day, intermittent, fasting, work, hi, eve...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Round 1 of cleaning with added stopwords (68 words added)\n",
    "stopwords_round_1 = ['everyon', 'didnt', 'tri', 'never', 'normal', \n",
    "                     'thank', 'say', 'post', 'use', 'els', \n",
    "                     'gain', 'thought', 'year', 'lose', 'past', \n",
    "                     'life', 'without', 'hope', 'cant', 'love', \n",
    "                     'sure', 'get', 'ago', 'week', 'comment', \n",
    "                     'around', 'meal', 'work', 'look', 'long', \n",
    "                     'littl', 'alway', 'start', 'right', 'thing', \n",
    "                     'end', 'stop', 'could', 'peopl', 'made', \n",
    "                     'went', 'want', 'almost', 'period', 'find', \n",
    "                     'make', 'advic', 'id', 'time', 'actual', \n",
    "                     'notic', 'two', 'hard', 'felt', 'come', \n",
    "                     'ill', 'pretti', 'healthi', 'anyth', 'enough', \n",
    "                     'etc', 'sometim', 'happi', 'mayb', 'hungri', \n",
    "                     'experi', 'less', 'that']\n",
    "\n",
    "reddit[['stemmed_round_1', 'lemmatized_round_1']] = reddit['title_&_text'].apply(clean, custom_stopwords=stopwords_round_1)\n",
    "\n",
    "# Round 2 of cleaning with added stopwords (20 words added)\n",
    "stopwords_round_2 = ['made', 'went', 'want', 'almost', 'period', \n",
    "                     'find', 'make', 'advic', 'id', 'time', \n",
    "                     'actual', 'notic', 'two', 'hard', 'felt', \n",
    "                     'come', 'ill', 'pretti', 'healthi', 'anyth']\n",
    "\n",
    "reddit[['stemmed_round_2', 'lemmatized_round_2']] = reddit['title_&_text'].apply(clean, custom_stopwords=stopwords_round_2)\n",
    "\n",
    "# Round 3 of cleaning with added stopwords (9 words added)\n",
    "stopwords_round_3 = ['enough', 'etc', 'sometim', 'happi', 'mayb', 'hungri', 'experi', 'less', 'that']\n",
    "\n",
    "reddit[['stemmed_round_3', 'lemmatized_round_3']] = reddit['title_&_text'].apply(clean, custom_stopwords=stopwords_round_3)\n",
    "\n",
    "reddit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5f741",
   "metadata": {},
   "source": [
    "> <font size = 3 color = \"crimson\"> While I see that you said that stop words were defined via most common words used, there are a few things that come to mind. The first is that some of the words being removed seem to me not to be contributing noise but rather to be crucial words such as 'period', or 'hope', or 'hungri' or 'pretti'. I say these are crucial words because these indicate some sentiment that I, as a casual layperson at least, would expect might have some relation to the mindset of users of these subreddits. Words that you remove should be meaningless words that don't tell you anything about the users. Good examples from your own stop list include words like 'want' or 'advic'.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b115c763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>total_comments</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_type</th>\n",
       "      <th>title_&amp;_text</th>\n",
       "      <th>stemmed_round_1</th>\n",
       "      <th>lemmatized_round_1</th>\n",
       "      <th>stemmed_round_2</th>\n",
       "      <th>lemmatized_round_2</th>\n",
       "      <th>stemmed_round_3</th>\n",
       "      <th>lemmatized_round_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>new</td>\n",
       "      <td>Does taking flavoured creatine break a fast? T...</td>\n",
       "      <td>[take, flavour, creatin, break, fast, take, on...</td>\n",
       "      <td>[taking, flavoured, creatine, break, fast, tak...</td>\n",
       "      <td>[take, flavour, creatin, break, fast, take, on...</td>\n",
       "      <td>[taking, flavoured, creatine, break, fast, tak...</td>\n",
       "      <td>[take, flavour, creatin, break, fast, take, on...</td>\n",
       "      <td>[taking, flavoured, creatine, break, fast, tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>new</td>\n",
       "      <td>I lost 120 lbs.......she lost 80. One meal a d...</td>\n",
       "      <td>[lost, 120, lbsshe, lost, 80, one, day, ]</td>\n",
       "      <td>[lost, 120, lbsshe, lost, 80, one, day, ]</td>\n",
       "      <td>[lost, 120, lbsshe, lost, 80, one, meal, day, ]</td>\n",
       "      <td>[lost, 120, lbsshe, lost, 80, one, meal, day, ]</td>\n",
       "      <td>[lost, 120, lbsshe, lost, 80, one, meal, day, ]</td>\n",
       "      <td>[lost, 120, lbsshe, lost, 80, one, meal, day, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>new</td>\n",
       "      <td>Does fasting out of spite work? We’ll see in 4...</td>\n",
       "      <td>[fast, spite, see, 4, week, go, wed, bh, siste...</td>\n",
       "      <td>[fasting, spite, see, 4, week, go, wedding, bh...</td>\n",
       "      <td>[fast, spite, work, see, 4, week, go, wed, bh,...</td>\n",
       "      <td>[fasting, spite, work, see, 4, week, go, weddi...</td>\n",
       "      <td>[fast, spite, work, see, 4, week, go, wed, bh,...</td>\n",
       "      <td>[fasting, spite, work, see, 4, week, go, weddi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>new</td>\n",
       "      <td>Daily Fasting Check-in! * **Type** of fast (wa...</td>\n",
       "      <td>[daili, fast, checkin, type, fast, water, juic...</td>\n",
       "      <td>[daily, fasting, checkin, type, fast, water, j...</td>\n",
       "      <td>[daili, fast, checkin, type, fast, water, juic...</td>\n",
       "      <td>[daily, fasting, checkin, type, fast, water, j...</td>\n",
       "      <td>[daili, fast, checkin, type, fast, water, juic...</td>\n",
       "      <td>[daily, fasting, checkin, type, fast, water, j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>new</td>\n",
       "      <td>90 Days of Intermittent Fasting - IT WORKS! Hi...</td>\n",
       "      <td>[90, day, intermitt, fast, work, hi, everyon, ...</td>\n",
       "      <td>[90, day, intermittent, fasting, work, hi, eve...</td>\n",
       "      <td>[90, day, intermitt, fast, work, hi, everyon, ...</td>\n",
       "      <td>[90, day, intermittent, fasting, work, hi, eve...</td>\n",
       "      <td>[90, day, intermitt, fast, work, hi, everyon, ...</td>\n",
       "      <td>[90, day, intermittent, fasting, work, hi, eve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score  total_comments            subreddit post_type  \\\n",
       "0      1               0  intermittentfasting       new   \n",
       "1      6               1  intermittentfasting       new   \n",
       "2      0               2  intermittentfasting       new   \n",
       "3      1               0  intermittentfasting       new   \n",
       "4     17               8  intermittentfasting       new   \n",
       "\n",
       "                                        title_&_text  \\\n",
       "0  Does taking flavoured creatine break a fast? T...   \n",
       "1  I lost 120 lbs.......she lost 80. One meal a d...   \n",
       "2  Does fasting out of spite work? We’ll see in 4...   \n",
       "3  Daily Fasting Check-in! * **Type** of fast (wa...   \n",
       "4  90 Days of Intermittent Fasting - IT WORKS! Hi...   \n",
       "\n",
       "                                     stemmed_round_1  \\\n",
       "0  [take, flavour, creatin, break, fast, take, on...   \n",
       "1          [lost, 120, lbsshe, lost, 80, one, day, ]   \n",
       "2  [fast, spite, see, 4, week, go, wed, bh, siste...   \n",
       "3  [daili, fast, checkin, type, fast, water, juic...   \n",
       "4  [90, day, intermitt, fast, work, hi, everyon, ...   \n",
       "\n",
       "                                  lemmatized_round_1  \\\n",
       "0  [taking, flavoured, creatine, break, fast, tak...   \n",
       "1          [lost, 120, lbsshe, lost, 80, one, day, ]   \n",
       "2  [fasting, spite, see, 4, week, go, wedding, bh...   \n",
       "3  [daily, fasting, checkin, type, fast, water, j...   \n",
       "4  [90, day, intermittent, fasting, work, hi, eve...   \n",
       "\n",
       "                                     stemmed_round_2  \\\n",
       "0  [take, flavour, creatin, break, fast, take, on...   \n",
       "1    [lost, 120, lbsshe, lost, 80, one, meal, day, ]   \n",
       "2  [fast, spite, work, see, 4, week, go, wed, bh,...   \n",
       "3  [daili, fast, checkin, type, fast, water, juic...   \n",
       "4  [90, day, intermitt, fast, work, hi, everyon, ...   \n",
       "\n",
       "                                  lemmatized_round_2  \\\n",
       "0  [taking, flavoured, creatine, break, fast, tak...   \n",
       "1    [lost, 120, lbsshe, lost, 80, one, meal, day, ]   \n",
       "2  [fasting, spite, work, see, 4, week, go, weddi...   \n",
       "3  [daily, fasting, checkin, type, fast, water, j...   \n",
       "4  [90, day, intermittent, fasting, work, hi, eve...   \n",
       "\n",
       "                                     stemmed_round_3  \\\n",
       "0  [take, flavour, creatin, break, fast, take, on...   \n",
       "1    [lost, 120, lbsshe, lost, 80, one, meal, day, ]   \n",
       "2  [fast, spite, work, see, 4, week, go, wed, bh,...   \n",
       "3  [daili, fast, checkin, type, fast, water, juic...   \n",
       "4  [90, day, intermitt, fast, work, hi, everyon, ...   \n",
       "\n",
       "                                  lemmatized_round_3  \n",
       "0  [taking, flavoured, creatine, break, fast, tak...  \n",
       "1    [lost, 120, lbsshe, lost, 80, one, meal, day, ]  \n",
       "2  [fasting, spite, work, see, 4, week, go, weddi...  \n",
       "3  [daily, fasting, checkin, type, fast, water, j...  \n",
       "4  [90, day, intermittent, fasting, work, hi, eve...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop columns that we do not need\n",
    "reddit = reddit.drop(columns=['title', 'post_text', 'time_uploaded', 'post_url', 'id']).copy()\n",
    "\n",
    "reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d59094e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data in 'reddit_cleaned_final.csv'\n",
    "reddit.to_csv('reddit_cleaned_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e10ed4",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** [Part 3 - Exploratory Data Analysis (EDA)](Part%203%20-%20Exploratory%20Data%20Analysis%20(EDA).ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
